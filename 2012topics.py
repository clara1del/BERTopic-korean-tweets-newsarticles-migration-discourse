#before starting: #conda activate env_full2

#refugeenewstitles.csv
#migrantnewstitles.csv
#immigrantnewstitles.csv
#marriagemigrantnewstitles.csv
#illmigrantnewstitles.csv
#womenmigrantnewstitles.csv
#foreignersnewstitles.csv
#foreignermigrantsnewstitles.csv
#irregularmigrantsnewstitles.csv
#immigrantworkersnewstitles.csv

#allnewsarticles.csv
#migrantnewsarticles.csv
#marriageandwomennewsarticles.csv


#ompare with: 
#cleanedillmigrantnewstitles.csv
#cleanedrefugeenewstitles.csv
#cleanedmarriagemigrantnewstitles.csv
#cleanedwomenmigrantnewstitles.csv

#alldescription.csv

#
# %% 

import re
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer
from konlpy.tag import Mecab
from bertopic import BERTopic
import matplotlib.pylab as plt
from bertopic.representation import MaximalMarginalRelevance
from umap import UMAP
from hdbscan import HDBSCAN
from bertopic.vectorizers import ClassTfidfTransformer

import pandas as pd
import glob
import os
import re  
import numpy as np
from datetime import datetime
import dateutil.parser as parser  


#df=df.set_axis(['Raw_Tweet', 'removed'], axis=1, inplace=False)[["Raw_Tweet"]].astype(str)


df=pd.read_csv('tweets2012.csv', parse_dates=['Date'], lineterminator='\n') 

df=df.dropna()

timestamps = df['Date'].tolist()



#make a function to clean text from unwanted characters
file = open('stopwords.txt', 'r')
words = file.read()
file.close()
stopwords = list(words.split())

def remove_stopwords(text):
    text = [word for word in text.lower().split() if word not in stopwords]
    text = ' '.join(text[0:])
    return text

def clean_text(text):
    #Remove hyper links
    text = re.sub(r'https?:\/\/\S+', ' ', text)
    #Remove @mentions
    text = re.sub(r'@[A-Za-z0-9]+', ' ', text) #(if the below works, we can erase this)
    # Remove user @ references and '#' from tweet
    text = re.sub(r'\@\w+|\#|\d+', ' ', text)
    # Remove noice
    text = re.sub(r'[-_:,\'"+RT]|[a-z]|[ğŸ“ğŸ“ŒğŸ’¬ğŸŒ]|[âœ”â–¶]|\[|\]|\*|\(|\)|\.\.\.+|[â€¦]|\.\.+', ' ', text)
    # Remove extra brakets
    text=text.strip()
    # Remove urls
    text = re.sub(r"http\S+|www\S+|https\S+", ' ', text, flags=re.MULTILINE)
    # Remove Stop words
    text=remove_stopwords(text)
    return text

# Apply the clean_text function to the 'Tweet' column
df['Tweets']=df['Raw_Tweet'].apply(clean_text)

df.head()


#documents = [line.strip() for line in df.cleanedtext]
documents = [line.strip() for line in df.Tweets]
preprocessed_documents = []

for line in tqdm(documents):
  if line and not line.replace(' ', '').isdecimal():
    preprocessed_documents.append(line)

text=''.join(preprocessed_documents[:10000])
text=text.split('.')

class CustomTokenizer:
    def __init__(self, tagger):
        self.tagger = tagger
    def __call__(self, sent):
        sent = sent[:1000000]
        word_tokens = self.tagger.nouns(sent) #word_tokens = self.tagger.morphs(sent)
        result = [word for word in word_tokens if len(word) > 1]
        return result
    
seed_topic_list = [['ì‹¤ì—…', 'ì¼ìë¦¬', 'ì‘ì—… ê²½ìŸ','ì·¨ì—… ê²½ìŸ','ì§ì¥ ìƒ','ê²½ìŸ'], #1. Common Arguments against Immigration: â€œimmigrants take jobs, lower wages, hurt the poorâ€ (ref: https://www.cato.org/blog/14-most-common-arguments-against-immigration-why-theyre-wrong)
                   ['ê±´ê°• ë³´í—˜', 'ë³‘ì›', 'ë³µì§€','ë³´ê±´ì˜ë£Œì‚¬ë¹„ìŠ¤', 'ë³´ê±´ì˜ë£Œ', 'ë³´ê±´ì˜ë£Œìš”êµ¬','ì˜ë£Œ í˜œíƒ',  'ë³´í—˜ í˜œíƒ',  'ì˜ë£Œ', 'ê±´ê°•', 'ì˜ë£Œí˜œíƒ','ë³´ê±´ì˜ë£Œì •ì±…','IHS', 'NHS','ì˜ë£Œ ì„œë¹„ìŠ¤', 'ì‚¬íšŒë³µì§€'], #2. Common Arguments against Immigration : â€œ abuse welfareâ€
                   ['ì„¸ê¸ˆ','ë¶ˆê²½ê¸°'], #3. Common Arguments against Immigration: â€œincrease budget deficit and government debtâ€
                   ['í•œêµ­ì–´ ì‹¤ë ¥', 'ë¬¸í™” êµë¥˜','í†µí•©','ì–¸ì–´ì¥ë²½', 'ë™í™”','ì‚¬íšŒí†µí•© í”„ë¡œê·¸ë¨','ê³µë™'], #4.Common Arguments against Immigration: â€œ donâ€™t assimilate, integrateâ€community
                   ['ë¶ˆë²•','ë²”ì£„','ìœ„í—˜', 'ì‚´ì¸', 'ê°•ë„', 'ì ˆë„', 'ë§¤ì¶˜', 'ë§ˆì•½', 'ì‚¬ì´ë²„ ë²”ì£„', 'í…”ë ˆë±…í‚¹ ì‚¬ê¸°', 'í”¼ì‹±', 'ì™¸êµ­ì¸ ë²”ì£„', 'ìœ„ì¡°ë²”', 'ë°€ìˆ˜í’ˆ', 'ì‚°ì—…ì—°ìˆ˜ìƒ ë²”ì£„'], #5. Common Arguments against Immigration: â€œsource of crimeâ€
                   ['í…ŒëŸ¬','í…ŒëŸ¬ë¶„ì','í…ŒëŸ¬ë¦¬ì¦˜','í…ŒëŸ¬ë¦¬ìŠ¤íŠ¸','ì‚¬ë³´íƒ€ì£¼'], #6.Common Arguments against Immigration: â€œterrorismâ€ 
                   ['ë¯¼ì¡±ì£¼ì˜','í•œêµ­ì  ê°€ì¹˜ê´€, í•œêµ­ì„±','êµ­ê°€ ì´ë¯¸ì§€','í•œêµ­ ì´ë¯¸ì§€'], #7.Common Arguments against Immigration: national sovereignty 
                   ['ì •ë¶€','ë²•ë¬´ë¶€','ìœ¤ì„ì—´','ë¬¸ì¬ì¸','ë°•ê·¼í˜œ','ì´ëª…ë°•','ë…¸ë¬´í˜„','ê¹€ëŒ€ì¤‘','ëŒ€í†µë ¹','í†µì¹˜'], #8.ruling class - government
                   ['êµ­ì ','êµí¬', 'ë¯¸êµ­', 'ì¼ë³¸', 'ì´ì§‘íŠ¸',  'ì£¼ì „ì','ê³ ë ¤ì¸', 'ëŸ¬ì‹œì•„','í‘ì¸','ì•„ë','ë¼í‹´ì•„ë©”ë¦¬ì¹´ ' , 'ë² íŠ¸ë‚¨', 'ë‘¥í¬', 'ë°±ì¸', 'ì¡°ì„ ì¡±', 'ëŸ¬ì‹œì•„ì¸', 'ë¯¸êµ­ì¸', 'ìœ ëŸ½ì¸', 'ì„œêµ¬ì¸','ì„œì–‘ì¸', 'ë™ë‚¨ì•„ì‹œì•„ì¸', 'ë™ë‚¨ì•„ì¸', 'ìš°ì¦ˆë²¡ì¸', 'ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„ ì´ì£¼', 'ì¤‘êµ­ì¸', 'ì¤‘êµ­', 'ì•„í”„ë¦¬ì¹´', 'ì¸ë„', 'ìš°í¬ë¼ì´ë‚˜',  'ì¤‘ë™', 'ëª½ê³¨ì¸', 'ëª½ê³¨', 'íƒˆë¶ì´ì£¼ë¯¼', 'ë¶í•œì´íƒˆì£¼ë¯¼'], #9.dividing the working class with identity politics:racism and ethnicity ì¸ì¢…
                   ['ì´ì£¼ì—¬ì„±','ì—¬ì„±ì´ì£¼ë…¸ë™ì','ì—¬ì„±','ì—¬ì', 'ì  ë”'], #10.dividing the working class with identity politics: gender discrimination ì„±ë³„
                   ['MTU', 'ì¡°í•©', 'ì´ì£¼ë…¸ë™í¬ë§ì„¼í„°', 'ì´ë¯¼ì ì„¼í„°', 'ì—°ëŒ€', 'ì´ì£¼ë…¸ë™ìë…¸ë™ì¡°í•©', 'ì´ì£¼ë…¸ì¡°', 'ì´ì£¼ë…¸ë™ì ë…¸ë™ì¡°í•©', 'ì´ì£¼ë¯¼ì„¼í„° ì¹œêµ¬','ìƒë‹´'], #11.uniting the working class with union
                   ['êµ­ì œê²°í˜¼', 'ì™¸êµ­ì¸ ì‹ ë¶€', 'ê²°í˜¼ì´ë¯¼ì', 'ê²°í˜¼ì´ë¯¼','ê²°í˜¼ì´ì£¼ì', 'ê²°í˜¼ ì¤‘ê°œì—…', 'ì´ë¯¼ì ë¶€ëª¨','ê²°í˜¼', 'ì´í˜¼', 'ì•„ë‚´', 'ë‚¨í¸', 'ì‹ ë¶€', 'ê°€ì • í­ë ¥', 'ê°€ì¡± í­ë ¥'], #12.dividing migrants by migrant status and place in the online migration debate: marriage
                   ['ê°€ì¡±','ë‹¤ë¬¸í™”ì£¼ì˜','ì™¸êµ­ì¸ ì•„ë™','ë‹¤ë¬¸í™” ê°€ì •','ì´ë¯¼ì ë¶€ëª¨', 'ì„ì‚°ë¶€','ì„ì‹ ','ì–´ë¦°ì´', 'ë¶€ëª¨ë‹˜','ë‹¤ë¬¸í™”ê°€ì¡±'], #13.Family
                   ['ë¬´ìŠ¬ë¦¼','ì´ìŠ¬ëŒêµë„','ì´ìŠ¬ëŒ','ë¬´ìŠ¬ë¦¬ë§ˆ'], #14. divide by religion
                   ['ì„ ìƒë‹˜','ì˜ì–´ ì„ ìƒë‹˜', 'ë¶€ì', 'ì‚¬ì—…ê°€','íˆ¬ìì','êµìˆ˜','ìƒìš© ë¹„ì'], #15.status: high payed worker
                   ['ë†ì¥','ê±´ì„¤','ì„ ë°•', 'ì–´ì—…','E9', 'ê³ ìš©í—ˆê°€ì œ', 'ì„œë¹„ìŠ¤ì—…', 'ë†ì¶•ì‚°ì—…', 'ê±´ì„¤ì—…','ì œì¡°ì—…','ê±´ì„¤ê³µì‚¬', 'ì‘ë¬¼ì¬ë°°ì—…','ì¶•ì‚°ì—…','ì–‘ì‹ì–´ì—…','ì†Œê¸ˆì±„ì·¨ì—…', ' ë¹„ì „ë¬¸ì·¨ì—…','ê±´ì„¤íê¸°ë¬¼ ì²˜ë¦¬ì—…','ìœ¡ì²´ë…¸ë™','ê³µì¥', 'ê±´ì„¤ë…¸ë™ì', 'ê³„ì ˆ ë…¸ë™ì', '3D ì—…ì¢…','ì‚°ì—…ì—°ìˆ˜ìƒ ì‹œìŠ¤í…œ'], #16.status: low payed workers in 3d industries
                   ['ê°€ì‚¬ë„ìš°ë¯¸', 'ì…ì£¼ë„ìš°ë¯¸', 'ìœ¡ì•„ë„ìš°ë¯¸' , 'ê°„ë³‘ë„ìš°ë¯¸', 'ë² ì´ë¹„ì‹œí„°', 'ì™¸êµ­ì¸ ê°€ì‚¬ë„ìš°ë¯¸', 'ëŒë´„ë„ìš°ë¯¸', 'ê°„ë³‘ì¸', 'ë„ìš°ë¯¸','ìš”ì‹ì—…','ì‹ì—…'], #17.status: gendered work in service industry, nurse, cleaning staff
                   ['ê´€ê´‘ê°', 'ì—¬í–‰', 'ë¬¸í™”', 'ì—¬í–‰ì','ê´€ê´‘'], #18.status: tourist
                   ['í•™ìƒ', 'í•™êµ', 'ëŒ€í•™êµ', 'ëŒ€í•™ìƒ', 'êµí™˜ í•™ìƒ','ìœ í•™ë¹„ì', 'ìœ í•™', 'ì–´í•™ì—°ìˆ˜', 'êµí™˜í•™ìƒ', 'ì—°êµ¬ìœ í•™'], #19.status: student
                   ['íƒˆë¶ì','ë¶í•œì´íƒˆì£¼ë¯¼','íƒˆë¶','íƒˆë¶ì','íƒˆë¶ë¯¼','ìƒˆí„°ë¯¼','ë¶í•œì´íƒˆì£¼ë¯¼'], #20.north korea refugees
                   ['ë¶ˆë²•ì²´ë¥˜ì','ë¶ˆë²•ì²´ë¥˜ ì™¸êµ­ì¸', 'ë¶ˆë²•ì²´ë¥˜', 'ë¯¸ë“±ë¡', 'ë¯¸ë“±ë¡ ì´ì£¼ì', 'ë¶ˆì³¬ì','ë¯¸ë“±ë¡ì™¸êµ­ì¸ê·¼ë¡œì', 'ì™¸êµ­ì¸ ë¶ˆë²• ê·¼ë¡œì','ë¬´í—ˆê°€ ë…¸ë™ì'], #21.status: undocumented immigration
                   ['ì˜ˆë©˜', 'ë¯¸ì–€ë§ˆ','íŒŒí‚¤ìŠ¤íƒ„','ë°©ê¸€ë¼ë°ì‹œ','ì—í‹°ì˜¤í”¼ì•„'], #22.status: refugee based on nationality
                   ['ë³€í˜¸ì‚¬', 'ë²•ì›','ë¹„ì', 'ë²•', 'ì´ë¯¼ë²•','ì‹œë¯¼ê¶Œ', 'ë…¸ë™ë²•','êµ­ì ë²•', 'ì´ë¯¼ ì •ì±…', 'ê·¼ë¡œê¸°ì¤€ë²•'], #23.immigration law
                   ['ì¶œì…êµ­ê´€ë¦¬ì†Œ', 'ë¹„ìì—°ì¥', 'ë¹„ììœ í˜•ë³€ê²½', 'ë¹„ìì‹ ì²­', 'ì˜ì£¼ê¶Œ', 'ì²´ë¥˜í—ˆê°€', 'ë¹„ì'], #24.administration
                   ['ì €ì„ê¸ˆ ë…¸ë™', 'ê°’ì‹¼ ë…¸ë™ì', 'ì €ì„ê¸ˆ', 'ìµœì €ì„ê¸ˆ','ê³ ìš©','ê³„ê¸‰','ìë³¸'], #25.wages and work, working class in capitalism
                   ['ë…¸ë™ ì°©ì·¨','ë‚¨ìš©', 'ì°©ì·¨', 'ì‚¬ê³ ', 'ì§ì¥ ê´´ë¡­í˜', 'ê´´ë¡­í˜', 'ê·¼ë¡œí™˜ê²½', 'ì‘ì—… ì¡°ê±´','í­ë ¥','ë…¸ì˜ˆ'], #26.human rights abuse
                   ['ë…¸ë™ì‹œê°„', 'ë³µì§€', 'ì‹œì„¤', 'ì‚°ì—…ì¬í•´', 'ì„ê¸ˆì²´ë¶ˆ', 'ì‹œê°„ì™¸ ìˆ˜ë‹¹', 'í•´ê³ ','ë¶€ìƒ ë³´ìƒ'], #27.working conditions
                   ['ê²½ì°°ë‹¨ì†', 'ë‹¨ì†','í•©ë™ë‹¨ì†', 'ì •ë¶€í•©ë™ë‹¨ì†', 'ë‹¨ì†ì¶”ë°©','ì¶”ë°©','ê°•ì œì¶”ë°©','ì™¸êµ­ì¸ë³´í˜¸ì†Œ','ê²½ì°°', 'ê°ì˜¥','êµ­ê²½ê²€ì‚¬','í•œêµ­ê²½ì°°','êµ¬ì†'], #28.police state, border, deportation, expulsion
                   ['ì°¨ë³„','ì°¨ë³„ê¸ˆì§€','ì™¸êµ­ì¸ í˜ì˜¤','ì†Œìˆ˜ì',' ë°°ì œ','ë¶ˆí‰ë“±','ê³„ì¸µ','ê³ ì •ê´€ë…','ë‚™ì¸','ì„ ì…ê²¬','ì¸ì¢…ì°¨ë³„','ê¸°ë³¸ê¶Œ', 'í‰ë“±', 'ë¶ˆí‰ë“±', 'í¸ê²¬','ì¸ê¶Œ'], #29.discrimination awareness 
                   ['ê²½ì œ', 'ê²½ì œì´ì£¼', 'ê²½ì œì  ì´ë“', 'ë…¸ë™ìˆ˜ìš”','ë…¸ë™ë ¥ ë¶€ì¡±','ì´ìœ¤'], #30.economy    
                   ['ë¹„ì','ì‹ ì²­','ë°œê¸‰'], #visa administration (added topic)
                   ['ê´€ê´‘','ì—¬í–‰'], #(added topic)
                   ['ë‰´ìŠ¤'], #(added topic)
                   ['ë§ëª… ì‹ ì²­ì','ë‚œë¯¼','í”¼ë‚œì'], #refugees
                   ['ì¹œêµ¬'], #(added topic)
                   ['ìë°œ'], #(added topic)
                   ['ì¢…ëª©'], #(added topic)
                   ['ì˜ì–´','í•œêµ­ì–´'], #(added topic)
                   ['ì™¸êµ­ì¸','ì™¸êµ­'], #(added topic)
                   ['í•œêµ­','í•œêµ­ì¸'], #(added topic)
                   ['ì¼ë³¸','ì¼ë³¸ì–´'], #(added topic)
                   ['ì´ì£¼ë…¸','ë™ì'], #(added topic)
                   ['ê°œì¸','ì‚¬ëŒ','ë‚˜ë¼','ì¹´ë“œ','ìƒê°']] #(added topic)

    
custom_tokenizer = CustomTokenizer(Mecab())
vectorizer = CountVectorizer(tokenizer=custom_tokenizer, max_features=3000)

representation_model = MaximalMarginalRelevance(diversity=0.3)

umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine') #reduce dimensionality
hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True) #Cluster reduced embeddings
ctfidf_model = ClassTfidfTransformer() # Create topic representation

model = BERTopic(embedding_model="sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens", \
                 vectorizer_model=vectorizer,
                 #representation_model=representation_model,
                 umap_model=umap_model,              # Step 2 - Reduce dimensionality
                 #hdbscan_model=hdbscan_model,        # Step 3 - Cluster reduced embeddings
                 #ctfidf_model=ctfidf_model,          # Step 5 - Extract topic words
                 seed_topic_list=seed_topic_list,
                 min_topic_size=35,
                 nr_topics=31,
                 #nr_topics="auto",
                 top_n_words=20,
                 calculate_probabilities=True)




topics, probs = model.fit_transform(documents) # Train your BERTopic model
new_topics = model.reduce_outliers(documents, topics) # Reduce outliers

# Use the "c-TF-IDF" strategy with a threshold
#new_topics = model.reduce_outliers(documents, new_topics , strategy="c-tf-idf", threshold=0.1)

# Reduce all outliers that are left with the "distributions" strategy
#new_topics = model.reduce_outliers(documents, topics, strategy="distributions")
model.update_topics(documents, topics=new_topics)

#ng20222avril.



#model.reduce_topics(documents, nr_topics=30)
#model.save('ngarticles.h5')
model.save('topics20122.h5')




for i in range(0, 55): 
  print(i,'ë²ˆì§¸ í† í”½ :', model.get_topic(i))


similar_topics, similarity = \
model.find_topics("ì—¬ì„±", top_n = 10) 

print("Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[0])))
print("Similarity Score: {}".format(similarity[0]))

print("\n Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[1])))
print("Similarity Score: {}".format(similarity[1]))

print("\n Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[2])))
print("Similarity Score: {}".format(similarity[2]))

#2012

# %%

model.get_representative_docs()
model.get_topic_info()
model.get_topics()
model.generate_topic_labels()


# %%
import chart_studio.plotly as py
import plotly.graph_objects as go


#chart_studio.tools.set_credentials_file(username='claraclara', api_key='KnIGBIGbrwbmg8q8bdpn')

###

fig = model.visualize_barchart()
fig.write_html("20122barchart.html")
fig.show()

fig = model.visualize_topics()
fig.write_html("20122topics.html")
fig.show()


fig = model.visualize_hierarchy()
fig.write_html("20122hierarchy.html")
fig.show()


fig = model.visualize_heatmap()
fig.write_html("20122heatmap.html")
fig.show()


fig = model.visualize_term_rank()
fig.write_html("20122termrank.html")
fig.show()


fig = model.visualize_term_rank(log_scale=True)
fig.write_html("20122logtermrank.html")
fig.show()




# %%

import chart_studio.plotly as py
import plotly.graph_objects as go


#chart_studio.tools.set_credentials_file(username='claraclara', api_key='KnIGBIGbrwbmg8q8bdpn')

topics_over_time = model.topics_over_time(documents, timestamps)
fig = model.visualize_topics_over_time(topics_over_time)
fig.write_html("20122topicstime.html")
fig.show()


# %%



topics_over_time = model.topics_over_time(documents, timestamps)
model.visualize_topics_over_time(topics_over_time, top_n_topics=30)
model.visualize_topics_over_time(topics_over_time)
#avril16.

model.visualize_topics()
model.visualize_distribution(probs[0])
model.get_representative_docs()
model.get_topic_info()
model.get_topics()
model.visualize_barchart()
model.visualize_hierarchy()
model.visualize_heatmap()
model.visualize_term_rank()
model.visualize_term_rank(log_scale=True)
model.generate_topic_labels()

#bert2021.


# %%
''''''
from bertopic import BERTopic
import gensim.corpora as corpora
from gensim.models.coherencemodel import CoherenceModel

topic_model = BERTopic(n_gram_range=(2, 3), min_topic_size=5)
topics, _ = topic_model.fit_transform(docs)
cleaned_docs = topic_model._preprocess_text(docs)
vectorizer = topic_model.vectorizer_model
analyzer = vectorizer.build_analyzer()
tokens = [analyzer(doc) for doc in cleaned_docs]
dictionary = corpora.Dictionary(tokens)
corpus = [dictionary.doc2bow(token) for token in tokens]
topics = topic_model.get_topics()
topics.pop(-1, None)
topic_words = [
    [word for word, _ in topic_model.get_topic(topic) if word != ""] for topic in topics
 ]
 topic_words = [[words for words, _ in topic_model.get_topic(topic)] 
            for topic in range(len(set(topics))-1)]
''''''

 # Evaluate
from bertopic import BERTopic
import gensim
from gensim import corpora
import gensim.corpora as corpora
from gensim.models.coherencemodel import CoherenceModel
coherence_model = CoherenceModel(topics=topics, 
                             texts=tokens, 
                             corpus=corpus,
                             dictionary=dictionary, 
                             coherence='c_v')
coherence = coherence_model.get_coherence()