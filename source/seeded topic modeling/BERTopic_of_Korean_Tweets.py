#code for visual studio code, python
#BERTopic model, in one step
#before starting, conda activate environment

# %% 

import re
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer
from konlpy.tag import Mecab
from bertopic import BERTopic
import matplotlib.pylab as plt
from bertopic.representation import MaximalMarginalRelevance
from umap import UMAP
from hdbscan import HDBSCAN
from bertopic.vectorizers import ClassTfidfTransformer
import glob
import os
from datetime import datetime
import dateutil.parser as parser  

df=pd.read_csv('tweets2012.csv', parse_dates=['Date'], lineterminator='\n')  #replace with own dataset

df=df.dropna()

timestamps = df['Date'].tolist()

#function to clean text from unwanted characters
file = open('stopwords.txt', 'r')  # list of Korean stopwords is in this github project
words = file.read()
file.close()
stopwords = list(words.split())

def remove_stopwords(text):
    text = [word for word in text.lower().split() if word not in stopwords]
    text = ' '.join(text[0:])
    return text

def clean_text(text):
    #Remove hyper links
    text = re.sub(r'https?:\/\/\S+', ' ', text)
    #Remove @mentions
    text = re.sub(r'@[A-Za-z0-9]+', ' ', text) #(if the below works, we can erase this)
    # Remove user @ references and '#' from tweet
    text = re.sub(r'\@\w+|\#|\d+', ' ', text)
    # Remove noise
    text = re.sub(r'[-_:,\'"+RT]|[a-z]|[ğŸ“ğŸ“ŒğŸ’¬ğŸŒ]|[âœ”â–¶]|\[|\]|\*|\(|\)|\.\.\.+|[â€¦]|\.\.+', ' ', text)
    # Remove extra brakets
    text=text.strip()
    # Remove urls
    text = re.sub(r"http\S+|www\S+|https\S+", ' ', text, flags=re.MULTILINE)
    # Remove Stop words
    text=remove_stopwords(text)
    return text

# Apply the clean_text function to the 'Tweets' column
df['Tweets']=df['Raw_Tweet'].apply(clean_text)

df.head()

documents = [line.strip() for line in df.Tweets]
preprocessed_documents = []

for line in tqdm(documents):
  if line and not line.replace(' ', '').isdecimal():
    preprocessed_documents.append(line)

text=''.join(preprocessed_documents[:10000])
text=text.split('.')

#We apply BERTopic to Korean text, and thus want to use Mecab as a tokenizer instead
class CustomTokenizer:
    def __init__(self, tagger):
        self.tagger = tagger
    def __call__(self, sent):
        sent = sent[:1000000]
        word_tokens = self.tagger.nouns(sent) #we keep only the nouns for clarity with tagger.nouns . To keep all words: word_tokens = self.tagger.morphs(sent)
        result = [word for word in word_tokens if len(word) > 1]
        return result

#This seed topic list pushes the model towards the defined topics, IF they exist. 

seed_topic_list = [['ì‹¤ì—…', 'ì¼ìë¦¬', 'ì‘ì—… ê²½ìŸ','ì·¨ì—… ê²½ìŸ','ì§ì¥ ìƒ','ê²½ìŸ'], #1. Common Arguments against Immigration: â€œimmigrants take jobs, lower wages, hurt the poorâ€ (ref: https://www.cato.org/blog/14-most-common-arguments-against-immigration-why-theyre-wrong)
                   ['ê±´ê°• ë³´í—˜', 'ë³‘ì›', 'ë³µì§€','ë³´ê±´ì˜ë£Œì‚¬ë¹„ìŠ¤', 'ë³´ê±´ì˜ë£Œ', 'ë³´ê±´ì˜ë£Œìš”êµ¬','ì˜ë£Œ í˜œíƒ',  'ë³´í—˜ í˜œíƒ',  'ì˜ë£Œ', 'ê±´ê°•', 'ì˜ë£Œí˜œíƒ','ë³´ê±´ì˜ë£Œì •ì±…','IHS', 'NHS','ì˜ë£Œ ì„œë¹„ìŠ¤', 'ì‚¬íšŒë³µì§€'], #2. Common Arguments against Immigration : â€œ abuse welfareâ€
                   ['ì„¸ê¸ˆ','ë¶ˆê²½ê¸°'], #3. Common Arguments against Immigration: â€œincrease budget deficit and government debtâ€
                   ['í•œêµ­ì–´ ì‹¤ë ¥', 'ë¬¸í™” êµë¥˜','í†µí•©','ì–¸ì–´ì¥ë²½', 'ë™í™”','ì‚¬íšŒí†µí•© í”„ë¡œê·¸ë¨','ê³µë™'], #4.Common Arguments against Immigration: â€œ donâ€™t assimilate, integrateâ€community
                   ['ë¶ˆë²•','ë²”ì£„','ìœ„í—˜', 'ì‚´ì¸', 'ê°•ë„', 'ì ˆë„', 'ë§¤ì¶˜', 'ë§ˆì•½', 'ì‚¬ì´ë²„ ë²”ì£„', 'í…”ë ˆë±…í‚¹ ì‚¬ê¸°', 'í”¼ì‹±', 'ì™¸êµ­ì¸ ë²”ì£„', 'ìœ„ì¡°ë²”', 'ë°€ìˆ˜í’ˆ', 'ì‚°ì—…ì—°ìˆ˜ìƒ ë²”ì£„'], #5. Common Arguments against Immigration: â€œsource of crimeâ€
                   ['í…ŒëŸ¬','í…ŒëŸ¬ë¶„ì','í…ŒëŸ¬ë¦¬ì¦˜','í…ŒëŸ¬ë¦¬ìŠ¤íŠ¸','ì‚¬ë³´íƒ€ì£¼'], #6.Common Arguments against Immigration: â€œterrorismâ€ 
                   ['ë¯¼ì¡±ì£¼ì˜','í•œêµ­ì  ê°€ì¹˜ê´€, í•œêµ­ì„±','êµ­ê°€ ì´ë¯¸ì§€','í•œêµ­ ì´ë¯¸ì§€'], #7.Common Arguments against Immigration: national sovereignty 
                   ['ì •ë¶€','ë²•ë¬´ë¶€','ìœ¤ì„ì—´','ë¬¸ì¬ì¸','ë°•ê·¼í˜œ','ì´ëª…ë°•','ë…¸ë¬´í˜„','ê¹€ëŒ€ì¤‘','ëŒ€í†µë ¹','í†µì¹˜'], #8.ruling class - government
                   ['êµ­ì ','êµí¬', 'ë¯¸êµ­', 'ì¼ë³¸', 'ì´ì§‘íŠ¸',  'ì£¼ì „ì','ê³ ë ¤ì¸', 'ëŸ¬ì‹œì•„','í‘ì¸','ì•„ë','ë¼í‹´ì•„ë©”ë¦¬ì¹´ ' , 'ë² íŠ¸ë‚¨', 'ë‘¥í¬', 'ë°±ì¸', 'ì¡°ì„ ì¡±', 'ëŸ¬ì‹œì•„ì¸', 'ë¯¸êµ­ì¸', 'ìœ ëŸ½ì¸', 'ì„œêµ¬ì¸','ì„œì–‘ì¸', 'ë™ë‚¨ì•„ì‹œì•„ì¸', 'ë™ë‚¨ì•„ì¸', 'ìš°ì¦ˆë²¡ì¸', 'ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„ ì´ì£¼', 'ì¤‘êµ­ì¸', 'ì¤‘êµ­', 'ì•„í”„ë¦¬ì¹´', 'ì¸ë„', 'ìš°í¬ë¼ì´ë‚˜',  'ì¤‘ë™', 'ëª½ê³¨ì¸', 'ëª½ê³¨', 'íƒˆë¶ì´ì£¼ë¯¼', 'ë¶í•œì´íƒˆì£¼ë¯¼'], #9.dividing the working class with identity politics:racism and ethnicity ì¸ì¢…
                   ['ì´ì£¼ì—¬ì„±','ì—¬ì„±ì´ì£¼ë…¸ë™ì','ì—¬ì„±','ì—¬ì', 'ì  ë”'], #10.dividing the working class with identity politics: gender discrimination ì„±ë³„
                   ['MTU', 'ì¡°í•©', 'ì´ì£¼ë…¸ë™í¬ë§ì„¼í„°', 'ì´ë¯¼ì ì„¼í„°', 'ì—°ëŒ€', 'ì´ì£¼ë…¸ë™ìë…¸ë™ì¡°í•©', 'ì´ì£¼ë…¸ì¡°', 'ì´ì£¼ë…¸ë™ì ë…¸ë™ì¡°í•©', 'ì´ì£¼ë¯¼ì„¼í„° ì¹œêµ¬','ìƒë‹´'], #11.uniting the working class with union
                   ['êµ­ì œê²°í˜¼', 'ì™¸êµ­ì¸ ì‹ ë¶€', 'ê²°í˜¼ì´ë¯¼ì', 'ê²°í˜¼ì´ë¯¼','ê²°í˜¼ì´ì£¼ì', 'ê²°í˜¼ ì¤‘ê°œì—…', 'ì´ë¯¼ì ë¶€ëª¨','ê²°í˜¼', 'ì´í˜¼', 'ì•„ë‚´', 'ë‚¨í¸', 'ì‹ ë¶€', 'ê°€ì • í­ë ¥', 'ê°€ì¡± í­ë ¥'], #12.dividing migrants by migrant status and place in the online migration debate: marriage
                   ['ê°€ì¡±','ë‹¤ë¬¸í™”ì£¼ì˜','ì™¸êµ­ì¸ ì•„ë™','ë‹¤ë¬¸í™” ê°€ì •','ì´ë¯¼ì ë¶€ëª¨', 'ì„ì‚°ë¶€','ì„ì‹ ','ì–´ë¦°ì´', 'ë¶€ëª¨ë‹˜','ë‹¤ë¬¸í™”ê°€ì¡±'], #13.Family
                   ['ë¬´ìŠ¬ë¦¼','ì´ìŠ¬ëŒêµë„','ì´ìŠ¬ëŒ','ë¬´ìŠ¬ë¦¬ë§ˆ'], #14. divide by religion
                   ['ì„ ìƒë‹˜','ì˜ì–´ ì„ ìƒë‹˜', 'ë¶€ì', 'ì‚¬ì—…ê°€','íˆ¬ìì','êµìˆ˜','ìƒìš© ë¹„ì'], #15.status: high payed worker
                   ['ë†ì¥','ê±´ì„¤','ì„ ë°•', 'ì–´ì—…','E9', 'ê³ ìš©í—ˆê°€ì œ', 'ì„œë¹„ìŠ¤ì—…', 'ë†ì¶•ì‚°ì—…', 'ê±´ì„¤ì—…','ì œì¡°ì—…','ê±´ì„¤ê³µì‚¬', 'ì‘ë¬¼ì¬ë°°ì—…','ì¶•ì‚°ì—…','ì–‘ì‹ì–´ì—…','ì†Œê¸ˆì±„ì·¨ì—…', ' ë¹„ì „ë¬¸ì·¨ì—…','ê±´ì„¤íê¸°ë¬¼ ì²˜ë¦¬ì—…','ìœ¡ì²´ë…¸ë™','ê³µì¥', 'ê±´ì„¤ë…¸ë™ì', 'ê³„ì ˆ ë…¸ë™ì', '3D ì—…ì¢…','ì‚°ì—…ì—°ìˆ˜ìƒ ì‹œìŠ¤í…œ'], #16.status: low payed workers in 3d industries
                   ['ê°€ì‚¬ë„ìš°ë¯¸', 'ì…ì£¼ë„ìš°ë¯¸', 'ìœ¡ì•„ë„ìš°ë¯¸' , 'ê°„ë³‘ë„ìš°ë¯¸', 'ë² ì´ë¹„ì‹œí„°', 'ì™¸êµ­ì¸ ê°€ì‚¬ë„ìš°ë¯¸', 'ëŒë´„ë„ìš°ë¯¸', 'ê°„ë³‘ì¸', 'ë„ìš°ë¯¸','ìš”ì‹ì—…','ì‹ì—…'], #17.status: gendered work in service industry, nurse, cleaning staff
                   ['ê´€ê´‘ê°', 'ì—¬í–‰', 'ë¬¸í™”', 'ì—¬í–‰ì','ê´€ê´‘'], #18.status: tourist
                   ['í•™ìƒ', 'í•™êµ', 'ëŒ€í•™êµ', 'ëŒ€í•™ìƒ', 'êµí™˜ í•™ìƒ','ìœ í•™ë¹„ì', 'ìœ í•™', 'ì–´í•™ì—°ìˆ˜', 'êµí™˜í•™ìƒ', 'ì—°êµ¬ìœ í•™'], #19.status: student
                   ['íƒˆë¶ì','ë¶í•œì´íƒˆì£¼ë¯¼','íƒˆë¶','íƒˆë¶ì','íƒˆë¶ë¯¼','ìƒˆí„°ë¯¼','ë¶í•œì´íƒˆì£¼ë¯¼'], #20.north korea refugees
                   ['ë¶ˆë²•ì²´ë¥˜ì','ë¶ˆë²•ì²´ë¥˜ ì™¸êµ­ì¸', 'ë¶ˆë²•ì²´ë¥˜', 'ë¯¸ë“±ë¡', 'ë¯¸ë“±ë¡ ì´ì£¼ì', 'ë¶ˆì³¬ì','ë¯¸ë“±ë¡ì™¸êµ­ì¸ê·¼ë¡œì', 'ì™¸êµ­ì¸ ë¶ˆë²• ê·¼ë¡œì','ë¬´í—ˆê°€ ë…¸ë™ì'], #21.status: undocumented immigration
                   ['ì˜ˆë©˜', 'ë¯¸ì–€ë§ˆ','íŒŒí‚¤ìŠ¤íƒ„','ë°©ê¸€ë¼ë°ì‹œ','ì—í‹°ì˜¤í”¼ì•„'], #22.status: refugee based on nationality
                   ['ë³€í˜¸ì‚¬', 'ë²•ì›','ë¹„ì', 'ë²•', 'ì´ë¯¼ë²•','ì‹œë¯¼ê¶Œ', 'ë…¸ë™ë²•','êµ­ì ë²•', 'ì´ë¯¼ ì •ì±…', 'ê·¼ë¡œê¸°ì¤€ë²•'], #23.immigration law
                   ['ì¶œì…êµ­ê´€ë¦¬ì†Œ', 'ë¹„ìì—°ì¥', 'ë¹„ììœ í˜•ë³€ê²½', 'ë¹„ìì‹ ì²­', 'ì˜ì£¼ê¶Œ', 'ì²´ë¥˜í—ˆê°€', 'ë¹„ì'], #24.administration
                   ['ì €ì„ê¸ˆ ë…¸ë™', 'ê°’ì‹¼ ë…¸ë™ì', 'ì €ì„ê¸ˆ', 'ìµœì €ì„ê¸ˆ','ê³ ìš©','ê³„ê¸‰','ìë³¸'], #25.wages and work, working class in capitalism
                   ['ë…¸ë™ ì°©ì·¨','ë‚¨ìš©', 'ì°©ì·¨', 'ì‚¬ê³ ', 'ì§ì¥ ê´´ë¡­í˜', 'ê´´ë¡­í˜', 'ê·¼ë¡œí™˜ê²½', 'ì‘ì—… ì¡°ê±´','í­ë ¥','ë…¸ì˜ˆ'], #26.human rights abuse
                   ['ë…¸ë™ì‹œê°„', 'ë³µì§€', 'ì‹œì„¤', 'ì‚°ì—…ì¬í•´', 'ì„ê¸ˆì²´ë¶ˆ', 'ì‹œê°„ì™¸ ìˆ˜ë‹¹', 'í•´ê³ ','ë¶€ìƒ ë³´ìƒ'], #27.working conditions
                   ['ê²½ì°°ë‹¨ì†', 'ë‹¨ì†','í•©ë™ë‹¨ì†', 'ì •ë¶€í•©ë™ë‹¨ì†', 'ë‹¨ì†ì¶”ë°©','ì¶”ë°©','ê°•ì œì¶”ë°©','ì™¸êµ­ì¸ë³´í˜¸ì†Œ','ê²½ì°°', 'ê°ì˜¥','êµ­ê²½ê²€ì‚¬','í•œêµ­ê²½ì°°','êµ¬ì†'], #28.police state, border, deportation, expulsion
                   ['ì°¨ë³„','ì°¨ë³„ê¸ˆì§€','ì™¸êµ­ì¸ í˜ì˜¤','ì†Œìˆ˜ì',' ë°°ì œ','ë¶ˆí‰ë“±','ê³„ì¸µ','ê³ ì •ê´€ë…','ë‚™ì¸','ì„ ì…ê²¬','ì¸ì¢…ì°¨ë³„','ê¸°ë³¸ê¶Œ', 'í‰ë“±', 'ë¶ˆí‰ë“±', 'í¸ê²¬','ì¸ê¶Œ'], #29.discrimination awareness 
                   ['ê²½ì œ', 'ê²½ì œì´ì£¼', 'ê²½ì œì  ì´ë“', 'ë…¸ë™ìˆ˜ìš”','ë…¸ë™ë ¥ ë¶€ì¡±','ì´ìœ¤'], #30.economy    
                   ['ë¹„ì','ì‹ ì²­','ë°œê¸‰'], #visa administration (added topic)
                   ['ê´€ê´‘','ì—¬í–‰'], #(added topic)
                   ['ë‰´ìŠ¤'], #(added topic)
                   ['ë§ëª… ì‹ ì²­ì','ë‚œë¯¼','í”¼ë‚œì'], #refugees
                   ['ì¹œêµ¬'], #(added topic)
                   ['ìë°œ'], #(added topic)
                   ['ì¢…ëª©'], #(added topic)
                   ['ì˜ì–´','í•œêµ­ì–´'], #(added topic)
                   ['ì™¸êµ­ì¸','ì™¸êµ­'], #(added topic)
                   ['í•œêµ­','í•œêµ­ì¸'], #(added topic)
                   ['ì¼ë³¸','ì¼ë³¸ì–´'], #(added topic)
                   ['ì´ì£¼ë…¸','ë™ì'], #(added topic)
                   ['ê°œì¸','ì‚¬ëŒ','ë‚˜ë¼','ì¹´ë“œ','ìƒê°']] #(added topic to decrease the overlapping of topics)

    
custom_tokenizer = CustomTokenizer(Mecab())
vectorizer = CountVectorizer(tokenizer=custom_tokenizer, max_features=3000)

representation_model = MaximalMarginalRelevance(diversity=0.3)

umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine') #reduce dimensionality # neighbords, components, can be updated to decrease topic overlapping

model = BERTopic(embedding_model="sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens", \  #we use this model for Korean text
                 vectorizer_model=vectorizer,
                 umap_model=umap_model,      # Reduce dimensionality
                 seed_topic_list=seed_topic_list,
                 min_topic_size=35, 
                 nr_topics=31, #can be changed to "auto"
                 top_n_words=20, 
                 calculate_probabilities=True)


topics, probs = model.fit_transform(documents) # Train the BERTopic model
new_topics = model.reduce_outliers(documents, topics) # Reduce outliers

model.save('topics2012.h5')

# %% 

#Now for the results

for i in range(0, 32): 
  print(i,'ë²ˆì§¸ í† í”½ :', model.get_topic(i))

# %% 
#To find the most similar topics for a specific word (here, "ì—¬ì„±" )

similar_topics, similarity = \
model.find_topics("ì—¬ì„±", top_n = 10) 

print("Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[0])))
print("Similarity Score: {}".format(similarity[0]))

print("\n Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[1])))
print("Similarity Score: {}".format(similarity[1]))

print("\n Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[2])))
print("Similarity Score: {}".format(similarity[2]))


# %%
model.get_representative_docs()
model.get_topic_info()
model.get_topics()
model.generate_topic_labels()

# %%
#Visualization of the results: how to plot an interactive graph html

import chart_studio.plotly as py
import plotly.graph_objects as go

fig = model.visualize_barchart()
fig.write_html("2012barchart.html")
fig.show()

fig = model.visualize_topics()
fig.write_html("2012topics.html")
fig.show()

fig = model.visualize_hierarchy()
fig.write_html("2012hierarchy.html")
fig.show()

fig = model.visualize_heatmap()
fig.write_html("2012heatmap.html")
fig.show()

fig = model.visualize_term_rank()
fig.write_html("2012termrank.html")
fig.show()

fig = model.visualize_term_rank(log_scale=True)
fig.write_html("2012logtermrank.html")
fig.show()


# %%
#the visualization of the topics overtime takes longer, so it is separated into another cell

import chart_studio.plotly as py
import plotly.graph_objects as go

topics_over_time = model.topics_over_time(documents, timestamps)
fig = model.visualize_topics_over_time(topics_over_time)
fig.write_html("2012topicstime.html")
fig.show()


# %%
#If we only want images, no interactvie graphs html, visualization can be done as:

topics_over_time = model.topics_over_time(documents, timestamps)
model.visualize_topics_over_time(topics_over_time, top_n_topics=10)
model.visualize_topics_over_time(topics_over_time)


model.visualize_topics()
model.visualize_distribution(probs[0])
model.get_representative_docs()
model.get_topic_info()
model.get_topics()
model.visualize_barchart()
model.visualize_hierarchy()
model.visualize_heatmap()
model.visualize_term_rank()
model.visualize_term_rank(log_scale=True)
model.generate_topic_labels()
