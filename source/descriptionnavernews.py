# -*- coding: utf-8 -*-
"""DescriptionNaverNews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17FQfhX_E666ExUrmMm5d2LtYFSq6m7C6
"""

#cleanedrefugeenewstitles.csv
#cleanedmigrantnewstitles.csv
#cleanedimmigrantnewstitles.csv
#cleanedmarriagemigrantnewstitles.csv
#cleanedillmigrantnewstitles.csv
#cleanedwomenmigrantnewstitles.csv
#cleanedforeignersnewstitles.csv
#cleanedforeignermigrantsnewstitles.csv
#cleanedirregularmigrantsnewstitles.csv
#cleanedimmigrantworkersnewstitles.csv


#cleanedillmigrantnewstitles.csv
#cleanedrefugeenewstitles.csv
#cleanedmarriagemigrantnewstitles.csv
#cleanedwomenmigrantnewstitles.csv


#cleanedmigrantnewstitles.csv
#cleanedimmigrantnewstitles.csv
#cleanedforeignersnewstitles.csv
#cleanedforeignermigrantsnewstitles.csv
#cleanedimmigrantworkersnewstitles.csv

import pandas as pd


df1 = pd.read_csv('/content/drive/MyDrive/migration/cleanedrefugeenewstitles.csv', lineterminator='\n')
df2 = pd.read_csv('/content/drive/MyDrive/migration/cleanedmigrantnewstitles.csv', lineterminator='\n')
df3 = pd.read_csv('/content/drive/MyDrive/migration/cleanedimmigrantnewstitles.csv', lineterminator='\n')
df4 = pd.read_csv('/content/drive/MyDrive/migration/cleanedmarriagemigrantnewstitles.csv', lineterminator='\n')
df5 = pd.read_csv('/content/drive/MyDrive/migration/cleanedillmigrantnewstitles.csv', lineterminator='\n')
df6 = pd.read_csv('/content/drive/MyDrive/migration/cleanedwomenmigrantnewstitles.csv', lineterminator='\n')
df7 = pd.read_csv('/content/drive/MyDrive/migration/cleanedforeignersnewstitles.csv', lineterminator='\n')
df8 = pd.read_csv('/content/drive/MyDrive/migration/cleanedforeignermigrantsnewstitles.csv', lineterminator='\n')
df9 = pd.read_csv('/content/drive/MyDrive/migration/cleanedirregularmigrantsnewstitles.csv', lineterminator='\n')
df10 = pd.read_csv('/content/drive/MyDrive/migration/cleanedimmigrantworkersnewstitles.csv', lineterminator='\n')



#df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10], axis=0, ignore_index=True, sort=True)
df = pd.concat([df4, df6], axis=0, ignore_index=True, sort=True)
df.head()

df.to_csv('/content/drive/MyDrive/migration/marriageandwomennewsarticles.csv')

#News scrape
'''

import os
import sys
import urllib.request


client_id = "CX9harkKYlwKQsq4kCg0"
client_secret = "gNYNwIlniZ"

file_path = "./tesst.json"

encText = urllib.parse.quote("ì´ì£¼ë…¸ë™ì")



url = "https://openapi.naver.com/v1/search/news.json?display=100&start=1000&sort=sim&query=" + encText


request = urllib.request.Request(url)
request.add_header("X-Naver-Client-Id",client_id)
request.add_header("X-Naver-Client-Secret",client_secret)
response = urllib.request.urlopen(request)
rescode = response.getcode()
if(rescode==200):
    response_body = response.read()
    print(response_body.decode('utf-8'))
else:
    print("Error Code:" + rescode)


import pandas as pd
df = pd.read_fwf('/content/drive/MyDrive/migration/ì´ì£¼ë…¸ë™ì.txt')

df.to_csv('/content/drive/MyDrive/migration/immigrantworkersnewstitles.csv')

refugeenewstitles.csv

migrantnewstitles.csv

immigrantnewstitles.csv

marriagemigrantnewstitles.csv

illmigrantnewstitles.csv

womenmigrantnewstitles.csv

foreignersnewstitles.csv

foreignermigrantsnewstitles.csv

irregularmigrantsnewstitles.csv

immigrantworkersnewstitles.csv

'''

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd


df = pd.read_csv('/content/drive/MyDrive/migration/immigrantworkersnewstitles.csv', lineterminator='\n')
#refugeenewstitles.csv
#migrantnewstitles.csv
#immigrantnewstitles.csv
#marriagemigrantnewstitles.csv
#illmigrantnewstitles.csv

#womenmigrantnewstitles.csv
#foreignersnewstitles.csv
#foreignermigrantsnewstitles.csv
#irregularmigrantsnewstitles.csv
#immigrantworkersnewstitles.csv


column_names = list(df.columns.values)

for column_headers in df.columns:
    print(column_headers)

df.head()

df = df.drop(df.columns[[0, 1, 2]], axis=1)
df.rename(columns={"Unnamed: 2": "newstitle"}, inplace=True)
df.head()

df2 = df[df['newstitle'].str.contains('description|pubDate', na = False)]
df2.head()

df2.to_csv('/content/drive/MyDrive/migration/newsarticles/datedimmigrantworkersnewstitles.csv')
#refugeenewstitles.csv
#migrantnewstitles.csv
#immigrantnewstitles.csv
#marriagemigrantnewstitles.csv
#illmigrantnewstitles.csv
#womenmigrantnewstitles.csv
#foreignersnewstitles.csv
#foreignermigrantsnewstitles.csv
#irregularmigrantsnewstitles.csv
#immigrantworkersnewstitles.csv

import pandas as pd


df = pd.read_csv('/content/drive/MyDrive/migration/newsarticles/datedimmigrantworkersnewstitles.csv', lineterminator='\n')
df.head()

df2.to_csv('/content/drive/MyDrive/migration/newsarticles/datedimmigrantworkersnewstitles.csv')
#refugeenewstitles.csv
#migrantnewstitles.csv
#immigrantnewstitles.csv
#marriagemigrantnewstitles.csv
#illmigrantnewstitles.csv
#womenmigrantnewstitles.csv
#foreignersnewstitles.csv
#foreignermigrantsnewstitles.csv
#irregularmigrantsnewstitles.csv
#immigrantworkersnewstitles.csv

#df2["pubDate"] = df2["newstitle"].shift()
#df2["pubDate"] = df2[df2['newstitle'].str.contains('pubDate', na=False)]
#df2.head()

#merge consecutive rows
#then divide by pubDate

df3 = df[df['newstitle'].str.contains('title|description', na = False)]
df3.head()

df3.to_csv('/content/drive/MyDrive/migration/titleimmigrantworkersnewstitles.csv')

#if want to try over time
#df2 = df.join(df.newstitle.shift(-1).rename('pubDate')).dropna()
#df2.head()

import pandas as pd
import glob
import os
import re
import numpy as np
from datetime import datetime
import dateutil.parser as parser

df = pd.read_csv('/content/drive/MyDrive/migration/titleimmigrantworkersnewstitles.csv', lineterminator='\n')
#df=df.set_axis(['newstitle', 'removed'], axis=1, inplace=False)[["newstitle"]].astype(str)


#make a function to clean text from unwanted characters
file = open('/content/drive/MyDrive/migration/stopwords.txt', 'r')
words = file.read()
file.close()
stopwords = list(words.split())

def remove_stopwords(text):
    text = [word for word in text.lower().split() if word not in stopwords]
    text = ''.join(text[0:])
    return text

def clean_text(text):
    #Remove hyper links
    text = re.sub(r'https?:\/\/\S+', ' ', text)
    #Remove @mentions
    text = re.sub(r'@[A-Za-z0-9]+', ' ', text) #(if the below works, we can erase this)
    # Remove user @ references and '#' from tweet
    text = re.sub(r'\@\w+|\#|\d+', '', text)
    # Remove noice
    text = re.sub(r'[-_:,\'"+RT]|[a-z]|[ğŸ“ğŸ“ŒğŸ’¬ğŸŒ]|[âœ”â–¶]|\[|\]|\*|\(|\)|\.\.\.+|[â€¦]|\.\.+', '', text)
    # Remove extra brakets
    text=text.strip()
    # Remove urls
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    # Remove Stop words
    text=remove_stopwords(text)
    return text

# Apply the clean_text function to the 'Tweet' column
df['cleanedtext']=df['newstitle'].apply(clean_text)

df.head()

df.to_csv('/content/drive/MyDrive/migration/cleanedimmigrantworkersnewstitles.csv')

###################now donwload and use on remote for bertopic

#trials but failed:

from wordcloud import WordCloud
from wordcloud import ImageColorGenerator
from wordcloud import STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/migration/cleanednewsarticles.csv', lineterminator='\n')
df.head()

data =df['cleanedtext']

text = " ".join(i for i in data)
stopwords = set(STOPWORDS)
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)
plt.figure( figsize=(15,10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
!pip install bertopic
!pip install -q konlpy
!pip install bertopic[visualization]
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
# %cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh

from google.colab import drive
drive.mount('/content/drive')

import re
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer
from konlpy.tag import Mecab
from bertopic import BERTopic

df = pd.read_csv('/content/drive/MyDrive/migration/cleanednewsarticles.csv', lineterminator='\n')

df=df.dropna()


documents = [line.strip() for line in df.cleanedtext]
preprocessed_documents = []

for line in tqdm(documents):
  if line and not line.replace(' ', '').isdecimal():
    preprocessed_documents.append(line)

text=''.join(preprocessed_documents[:10000])
text=text.split('.')

class CustomTokenizer:
    def __init__(self, tagger):
        self.tagger = tagger
    def __call__(self, sent):
        sent = sent[:1000000]
        #word_tokens = self.tagger.morphs(sent)
        word_tokens = self.tagger.nouns(sent) #to have only nouns
        result = [word for word in word_tokens if len(word) > 1]
        return result

from bertopic import BERTopic
import pandas as pd
'''
seed_topic_list = [['ì‹¤ì—…', 'ì¼ìë¦¬', 'ì‘ì—… ê²½ìŸ','ì·¨ì—… ê²½ìŸ','ì§ì¥ ìƒ','ê²½ìŸ'], #1. Common Arguments against Immigration: â€œimmigrants take jobs, lower wages, hurt the poorâ€ (ref: https://www.cato.org/blog/14-most-common-arguments-against-immigration-why-theyre-wrong)
                   ['ê±´ê°• ë³´í—˜', 'ë³‘ì›', 'ë³µì§€','ë³´ê±´ì˜ë£Œì‚¬ë¹„ìŠ¤', 'ë³´ê±´ì˜ë£Œ', 'ë³´ê±´ì˜ë£Œìš”êµ¬','ì˜ë£Œ í˜œíƒ',  'ë³´í—˜ í˜œíƒ',  'ì˜ë£Œ', 'ê±´ê°•', 'ì˜ë£Œí˜œíƒ','ë³´ê±´ì˜ë£Œì •ì±…','IHS', 'NHS','ì˜ë£Œ ì„œë¹„ìŠ¤', 'ì‚¬íšŒë³µì§€'], #2. Common Arguments against Immigration : â€œ abuse welfareâ€
                   ['ì„¸ê¸ˆ','ë¶ˆê²½ê¸°'], #3. Common Arguments against Immigration: â€œincrease budget deficit and government debtâ€
                   ['í•œêµ­ì–´ ì‹¤ë ¥', 'ë¬¸í™” êµë¥˜','í†µí•©','ì–¸ì–´ì¥ë²½', 'ë™í™”','ì‚¬íšŒí†µí•© í”„ë¡œê·¸ë¨','ê³µë™'], #4.Common Arguments against Immigration: â€œ donâ€™t assimilate, integrateâ€community
                   ['ë¶ˆë²•','ë²”ì£„','ìœ„í—˜', 'ì‚´ì¸', 'ê°•ë„', 'ì ˆë„', 'ë§¤ì¶˜', 'ë§ˆì•½', 'ì‚¬ì´ë²„ ë²”ì£„', 'í…”ë ˆë±…í‚¹ ì‚¬ê¸°', 'í”¼ì‹±', 'ì™¸êµ­ì¸ ë²”ì£„', 'ìœ„ì¡°ë²”', 'ë°€ìˆ˜í’ˆ', 'ì‚°ì—…ì—°ìˆ˜ìƒ ë²”ì£„'], #5. Common Arguments against Immigration: â€œsource of crimeâ€
                   ['í…ŒëŸ¬','í…ŒëŸ¬ë¶„ì','í…ŒëŸ¬ë¦¬ì¦˜','í…ŒëŸ¬ë¦¬ìŠ¤íŠ¸','ì‚¬ë³´íƒ€ì£¼'], #6.Common Arguments against Immigration: â€œterrorismâ€
                   ['ë¯¼ì¡±ì£¼ì˜','í•œêµ­ì  ê°€ì¹˜ê´€, í•œêµ­ì„±','êµ­ê°€ ì´ë¯¸ì§€','í•œêµ­ ì´ë¯¸ì§€'], #7.Common Arguments against Immigration: national sovereignty
                   ['ì •ë¶€','ë²•ë¬´ë¶€','ìœ¤ì„ì—´','ë¬¸ì¬ì¸','ë°•ê·¼í˜œ','ì´ëª…ë°•','ë…¸ë¬´í˜„','ê¹€ëŒ€ì¤‘','ëŒ€í†µë ¹','í†µì¹˜'], #8.ruling class - government
                   ['êµ­ì ','êµí¬', 'ë¯¸êµ­', 'ì¼ë³¸', 'ì´ì§‘íŠ¸',  'ì£¼ì „ì','ê³ ë ¤ì¸', 'ëŸ¬ì‹œì•„','í‘ì¸','ì•„ë','ë¼í‹´ì•„ë©”ë¦¬ì¹´ ' , 'ë² íŠ¸ë‚¨', 'ë‘¥í¬', 'ë°±ì¸', 'ì¡°ì„ ì¡±', 'ëŸ¬ì‹œì•„ì¸', 'ë¯¸êµ­ì¸', 'ìœ ëŸ½ì¸', 'ì„œêµ¬ì¸','ì„œì–‘ì¸', 'ë™ë‚¨ì•„ì‹œì•„ì¸', 'ë™ë‚¨ì•„ì¸', 'ìš°ì¦ˆë²¡ì¸', 'ìš°ì¦ˆë² í‚¤ìŠ¤íƒ„ ì´ì£¼', 'ì¤‘êµ­ì¸', 'ì¤‘êµ­', 'ì•„í”„ë¦¬ì¹´', 'ì¸ë„', 'ìš°í¬ë¼ì´ë‚˜',  'ì¤‘ë™', 'ëª½ê³¨ì¸', 'ëª½ê³¨', 'íƒˆë¶ì´ì£¼ë¯¼', 'ë¶í•œì´íƒˆì£¼ë¯¼'], #9.dividing the working class with identity politics:racism and ethnicity ì¸ì¢…
                   ['ì´ì£¼ì—¬ì„±','ì—¬ì„±ì´ì£¼ë…¸ë™ì','ì—¬ì„±','ì—¬ì', 'ì  ë”'], #10.dividing the working class with identity politics: gender discrimination ì„±ë³„
                   ['MTU', 'ì¡°í•©', 'ì´ì£¼ë…¸ë™í¬ë§ì„¼í„°', 'ì´ë¯¼ì ì„¼í„°', 'ì—°ëŒ€', 'ì´ì£¼ë…¸ë™ìë…¸ë™ì¡°í•©', 'ì´ì£¼ë…¸ì¡°', 'ì´ì£¼ë…¸ë™ì ë…¸ë™ì¡°í•©', 'ì´ì£¼ë¯¼ì„¼í„° ì¹œêµ¬','ìƒë‹´'], #11.uniting the working class with union
                   ['êµ­ì œê²°í˜¼', 'ì™¸êµ­ì¸ ì‹ ë¶€', 'ê²°í˜¼ì´ë¯¼ì', 'ê²°í˜¼ì´ë¯¼','ê²°í˜¼ì´ì£¼ì', 'ê²°í˜¼ ì¤‘ê°œì—…', 'ì´ë¯¼ì ë¶€ëª¨','ê²°í˜¼', 'ì´í˜¼', 'ì•„ë‚´', 'ë‚¨í¸', 'ì‹ ë¶€', 'ê°€ì • í­ë ¥', 'ê°€ì¡± í­ë ¥'], #12.dividing migrants by migrant status and place in the online migration debate: marriage
                   ['ê°€ì¡±','ë‹¤ë¬¸í™”ì£¼ì˜','ì™¸êµ­ì¸ ì•„ë™','ë‹¤ë¬¸í™” ê°€ì •','ì´ë¯¼ì ë¶€ëª¨', 'ì„ì‚°ë¶€','ì„ì‹ ','ì–´ë¦°ì´', 'ë¶€ëª¨ë‹˜','ë‹¤ë¬¸í™”ê°€ì¡±'], #13.Family
                   ['ë¬´ìŠ¬ë¦¼','ì´ìŠ¬ëŒêµë„','ì´ìŠ¬ëŒ','ë¬´ìŠ¬ë¦¬ë§ˆ'], #14. divide by religion
                   ['ì„ ìƒë‹˜','ì˜ì–´ ì„ ìƒë‹˜', 'ë¶€ì', 'ì‚¬ì—…ê°€','íˆ¬ìì','êµìˆ˜','ìƒìš© ë¹„ì'], #15.status: high payed worker
                   ['ë†ì¥','ê±´ì„¤','ì„ ë°•', 'ì–´ì—…','E9', 'ê³ ìš©í—ˆê°€ì œ', 'ì„œë¹„ìŠ¤ì—…', 'ë†ì¶•ì‚°ì—…', 'ê±´ì„¤ì—…','ì œì¡°ì—…','ê±´ì„¤ê³µì‚¬', 'ì‘ë¬¼ì¬ë°°ì—…','ì¶•ì‚°ì—…','ì–‘ì‹ì–´ì—…','ì†Œê¸ˆì±„ì·¨ì—…', ' ë¹„ì „ë¬¸ì·¨ì—…','ê±´ì„¤íê¸°ë¬¼ ì²˜ë¦¬ì—…','ìœ¡ì²´ë…¸ë™','ê³µì¥', 'ê±´ì„¤ë…¸ë™ì', 'ê³„ì ˆ ë…¸ë™ì', '3D ì—…ì¢…','ì‚°ì—…ì—°ìˆ˜ìƒ ì‹œìŠ¤í…œ'], #16.status: low payed workers in 3d industries
                   ['ê°€ì‚¬ë„ìš°ë¯¸', 'ì…ì£¼ë„ìš°ë¯¸', 'ìœ¡ì•„ë„ìš°ë¯¸' , 'ê°„ë³‘ë„ìš°ë¯¸', 'ë² ì´ë¹„ì‹œí„°', 'ì™¸êµ­ì¸ ê°€ì‚¬ë„ìš°ë¯¸', 'ëŒë´„ë„ìš°ë¯¸', 'ê°„ë³‘ì¸', 'ë„ìš°ë¯¸','ìš”ì‹ì—…','ì‹ì—…'], #17.status: gendered work in service industry, nurse, cleaning staff
                   ['ê´€ê´‘ê°', 'ì—¬í–‰', 'ë¬¸í™”', 'ì—¬í–‰ì','ê´€ê´‘'], #18.status: tourist
                   ['í•™ìƒ', 'í•™êµ', 'ëŒ€í•™êµ', 'ëŒ€í•™ìƒ', 'êµí™˜ í•™ìƒ','ìœ í•™ë¹„ì', 'ìœ í•™', 'ì–´í•™ì—°ìˆ˜', 'êµí™˜í•™ìƒ', 'ì—°êµ¬ìœ í•™'], #19.status: student
                   ['íƒˆë¶ì','ë¶í•œì´íƒˆì£¼ë¯¼','íƒˆë¶','íƒˆë¶ì','íƒˆë¶ë¯¼','ìƒˆí„°ë¯¼','ë¶í•œì´íƒˆì£¼ë¯¼'], #20.north korea refugees
                   ['ë¶ˆë²•ì²´ë¥˜ì','ë¶ˆë²•ì²´ë¥˜ ì™¸êµ­ì¸', 'ë¶ˆë²•ì²´ë¥˜', 'ë¯¸ë“±ë¡', 'ë¯¸ë“±ë¡ ì´ì£¼ì', 'ë¶ˆì³¬ì','ë¯¸ë“±ë¡ì™¸êµ­ì¸ê·¼ë¡œì', 'ì™¸êµ­ì¸ ë¶ˆë²• ê·¼ë¡œì','ë¬´í—ˆê°€ ë…¸ë™ì'], #21.status: undocumented immigration
                   ['ë‚œë¯¼','í”¼ë‚œì', 'ì˜ˆë©˜', 'ë¯¸ì–€ë§ˆ','íŒŒí‚¤ìŠ¤íƒ„','ë°©ê¸€ë¼ë°ì‹œ','ì—í‹°ì˜¤í”¼ì•„','ë§ëª… ì‹ ì²­ì'], #22.status: refugee
                   ['ë³€í˜¸ì‚¬', 'ë²•ì›','ë¹„ì', 'ë²•', 'ì´ë¯¼ë²•','ì‹œë¯¼ê¶Œ', 'ë…¸ë™ë²•','êµ­ì ë²•', 'ì´ë¯¼ ì •ì±…', 'ê·¼ë¡œê¸°ì¤€ë²•'], #23.immigration law
                   ['ì¶œì…êµ­ê´€ë¦¬ì†Œ', 'ë¹„ìì—°ì¥', 'ë¹„ììœ í˜•ë³€ê²½', 'ë¹„ìì‹ ì²­', 'ì˜ì£¼ê¶Œ', 'ì²´ë¥˜í—ˆê°€', 'ë¹„ì'], #24.administration
                   ['ì €ì„ê¸ˆ ë…¸ë™', 'ê°’ì‹¼ ë…¸ë™ì', 'ì €ì„ê¸ˆ', 'ìµœì €ì„ê¸ˆ','ê³ ìš©','ê³„ê¸‰','ìë³¸'], #25.wages and work, working class in capitalism
                   ['ë…¸ë™ ì°©ì·¨','ë‚¨ìš©', 'ì°©ì·¨', 'ì‚¬ê³ ', 'ì§ì¥ ê´´ë¡­í˜', 'ê´´ë¡­í˜', 'ê·¼ë¡œí™˜ê²½', 'ì‘ì—… ì¡°ê±´','í­ë ¥','ë…¸ì˜ˆ'], #26.human rights abuse
                   ['ë…¸ë™ì‹œê°„', 'ë³µì§€', 'ì‹œì„¤', 'ì‚°ì—…ì¬í•´', 'ì„ê¸ˆì²´ë¶ˆ', 'ì‹œê°„ì™¸ ìˆ˜ë‹¹', 'í•´ê³ ','ë¶€ìƒ ë³´ìƒ'], #27.working conditions
                   ['ê²½ì°°ë‹¨ì†', 'ë‹¨ì†','í•©ë™ë‹¨ì†', 'ì •ë¶€í•©ë™ë‹¨ì†', 'ë‹¨ì†ì¶”ë°©','ì¶”ë°©','ê°•ì œì¶”ë°©','ì™¸êµ­ì¸ë³´í˜¸ì†Œ','ê²½ì°°', 'ê°ì˜¥','êµ­ê²½ê²€ì‚¬','í•œêµ­ê²½ì°°','êµ¬ì†'], #28.police state, border, deportation, expulsion
                   ['ì°¨ë³„','ì°¨ë³„ê¸ˆì§€','ì™¸êµ­ì¸ í˜ì˜¤','ì†Œìˆ˜ì',' ë°°ì œ','ë¶ˆí‰ë“±','ê³„ì¸µ','ê³ ì •ê´€ë…','ë‚™ì¸','ì„ ì…ê²¬','ì¸ì¢…ì°¨ë³„','ê¸°ë³¸ê¶Œ', 'í‰ë“±', 'ë¶ˆí‰ë“±', 'í¸ê²¬','ì¸ê¶Œ'], #29.discrimination awareness
                   ['ê²½ì œ', 'ê²½ì œì´ì£¼', 'ê²½ì œì  ì´ë“', 'ë…¸ë™ìˆ˜ìš”','ë…¸ë™ë ¥ ë¶€ì¡±','ì´ìœ¤']] #30.economy

 '''
custom_tokenizer = CustomTokenizer(Mecab())
vectorizer = CountVectorizer(tokenizer=custom_tokenizer, max_features=3000)

from bertopic import BERTopic

model = BERTopic(embedding_model="sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens", \
                 vectorizer_model=vectorizer,
                 #seed_topic_list=seed_topic_list,
                 min_topic_size=35,
                 nr_topics=31,
                 top_n_words=10,
                 calculate_probabilities=True)

topics, probs = model.fit_transform(text)

model.save('/content/drive/MyDrive/migration/mars7model.h5')

model.load('/content/drive/MyDrive/migration/mars7model.h5')

topics_over_time = model.topics_over_time(tweets, timestamps)

model.visualize_topics_over_time(topics_over_time, top_n_topics=30)

model.visualize_topics()
#model.visualize_topics(top_n_topics=10)

model.visualize_distribution(probs[0])

model.get_representative_docs()

model.get_representative_docs(1)

for i in range(0, 30):
  print(i,'ë²ˆì§¸ í† í”½ :', model.get_topic(i))

model.get_topic_info()

model.get_topics() #access all topics

model.visualize_hierarchy() #topic hierarchy

model.visualize_barchart() #Visualize Topic Terms

model.visualize_heatmap() #Visualize Topic Similarity

model.visualize_term_rank() #Visualize Term Score Decline

model.generate_topic_labels() #Generate topic labels

#Find topics that are similar to specified term: return top x topics that are semantically most similar to an input query term
#from: https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-iv-bertopic-46ddf3c91622

# x most similar topics to specified word
similar_topics, similarity = \
model.find_topics("ì—¬ì„±", top_n = 10)

print("Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[0])))
print("Similarity Score: {}".format(similarity[0]))

print("\n Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[1])))
print("Similarity Score: {}".format(similarity[1]))

print("\n Most Similar Topic Info: \n{}".format(model.get_topic(similar_topics[2])))
print("Similarity Score: {}".format(similarity[2]))